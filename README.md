# Serveur d'Actions Rasa - GÃ©nÃ©rateur de Visualisations avec Ollama

Un serveur d'actions Rasa qui gÃ©nÃ¨re automatiquement des plans de visualisation de donnÃ©es en utilisant des modÃ¨les de langage locaux via Ollama.

## ğŸ¯ Vue d'ensemble

Ce projet fournit un serveur d'actions Rasa capable de :
- Recevoir des requÃªtes en langage naturel pour crÃ©er des visualisations
- GÃ©nÃ©rer des plans d'analyse structurÃ©s en JSON
- Utiliser des LLM locaux via Ollama (pas de dÃ©pendance cloud)
- Supporter diffÃ©rents types de graphiques (barres, lignes, secteurs, etc.)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client Web    â”‚â”€â”€â”€â–¶â”‚  Serveur Rasa    â”‚â”€â”€â”€â–¶â”‚     Ollama      â”‚
â”‚   (Port 3000)   â”‚    â”‚   (Port 5055)    â”‚    â”‚ (Port 11434)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚ Plans JSON de    â”‚
                       â”‚ Visualisation    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Installation rapide

### PrÃ©requis
- Docker et Docker Compose
- Ollama installÃ© et configurÃ©
- Python 3.10+

### 1. Cloner le projet
```bash
git clone <repository-url>
cd rasa-visualization-server
```

### 2. Configurer Ollama
```bash
# Installer Ollama (si pas dÃ©jÃ  fait)
curl -fsSL https://ollama.ai/install.sh | sh

# TÃ©lÃ©charger le modÃ¨le recommandÃ©
ollama pull llama3.2:1b

# VÃ©rifier que Ollama fonctionne
ollama list
```

### 3. Configuration
CrÃ©er le fichier `.env` (ou modifier l'existant) :
```bash
# Configuration Ollama (Local)
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=llama3.2:1b
```

### 4. Lancer le serveur
```bash
# Dans le container de dÃ©veloppement
python -m rasa_sdk --actions src.actions

# Ou avec la tÃ¢che VS Code
# Ctrl+Shift+P > "Tasks: Run Task" > "Start Rasa Actions"
```

Le serveur sera accessible sur `http://localhost:5055`

## ğŸ“¡ API Usage

### Endpoint principal
```
POST http://localhost:5055/webhook
Content-Type: application/json
```

### Format de requÃªte
```json
{
  "next_action": "action_generate_visualization",
  "tracker": {
    "sender_id": "user123",
    "slots": {},
    "latest_message": {
      "text": "CrÃ©er un graphique en barres des ventes par rÃ©gion",
      "intent": {"name": "generate_chart"},
      "entities": []
    },
    "events": []
  },
  "domain": {}
}
```

### Exemples de requÃªtes supportÃ©es

#### Graphique en barres
```json
{
  "text": "GÃ©nÃ©rer un graphique en barres des ventes par rÃ©gion"
}
```

#### Graphique temporel
```json
{
  "text": "CrÃ©er un graphique en ligne des revenus sur 12 mois"
}
```

#### Graphique en secteurs
```json
{
  "text": "Faire un graphique en secteurs de la rÃ©partition des clients par Ã¢ge"
}
```

### Format de rÃ©ponse
```json
{
  "events": [],
  "responses": [{
    "text": "{\"charts\": [{\"title\": \"Ventes par rÃ©gion\", \"chart_type\": \"BAR\", \"metrics\": [...]}]}"
  }]
}
```

## ğŸ§ª Tests

### Test rapide avec curl
```bash
curl -X POST http://localhost:5055/webhook \
  -H "Content-Type: application/json" \
  -d '{
    "next_action": "action_generate_visualization",
    "tracker": {
      "sender_id": "test_user",
      "slots": {},
      "latest_message": {
        "text": "CrÃ©er un graphique simple",
        "intent": {"name": "generate_chart"},
        "entities": []
      },
      "events": []
    },
    "domain": {}
  }'
```

### Scripts de test inclus
```bash
# Test de base
python debug_parsing.py

# Test avec diffÃ©rents types de graphiques
python test_schema_validation.py

# Test complet Ollama
python test_ollama_fixed.py
```

## ğŸ”§ Configuration avancÃ©e

### Variables d'environnement
| Variable | Description | Valeur par dÃ©faut |
|----------|-------------|-------------------|
| `OLLAMA_BASE_URL` | URL du serveur Ollama | `http://ollama:11434` |
| `OLLAMA_MODEL` | ModÃ¨le Ollama Ã  utiliser | `llama3.2:1b` |

### ModÃ¨les Ollama recommandÃ©s
| ModÃ¨le | Taille | Performance | Usage |
|--------|--------|-------------|--------|
| `llama3.2:1b` | ~1GB | Rapide | DÃ©veloppement/test |
| `llama3.2:3b` | ~3GB | Ã‰quilibrÃ© | Production lÃ©gÃ¨re |
| `llama3.1:8b` | ~8GB | Haute | Production avancÃ©e |

### Personnaliser les types de graphiques
Modifier `/src/shared/SSOT/ChartType.yml` pour ajouter de nouveaux types :
```yaml
- canonical: "CUSTOM_CHART"
  description: "Mon type de graphique personnalisÃ©"
```

## ğŸ“ Structure du projet

```
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ actions.py              # Action Rasa principale
â”‚   â”œâ”€â”€ env.py                  # Configuration Ollama
â”‚   â”œâ”€â”€ langchain/
â”‚   â”‚   â”œâ”€â”€ planner_chain.py    # GÃ©nÃ©ration des plans LLM
â”‚   â”‚   â”œâ”€â”€ planner_examples.py # Exemples pour le prompt
â”‚   â”‚   â””â”€â”€ planner_schema.py   # SchÃ©mas Pydantic
â”‚   â””â”€â”€ shared/SSOT/            # DÃ©finitions des types de donnÃ©es
â”œâ”€â”€ test_*.py                   # Scripts de test
â”œâ”€â”€ .env                        # Configuration
â””â”€â”€ README.md                   # Ce fichier
```

## ğŸ› DÃ©pannage

### Erreur "Connection refused" sur le port 5055
```bash
# VÃ©rifier que le serveur est lancÃ©
ps aux | grep rasa_sdk

# Relancer le serveur
python -m rasa_sdk --actions src.actions
```

### ProblÃ¨mes de rÃ©seau Docker (containers multiples)
Si vous avez plusieurs containers Docker, vÃ©rifiez :

```bash
# 1. Lister les containers et leurs ports
docker ps --format "table {{.Names}}\t{{.Ports}}\t{{.Networks}}"

# 2. VÃ©rifier dans quel container le serveur d'actions fonctionne
docker exec -it action_devcontainer-action-1 ps aux | grep rasa_sdk

# 3. Tester la connectivitÃ© rÃ©seau entre containers
docker exec -it rasa_devcontainer-rasa-1 curl http://action:5055/health

# 4. Lancer le serveur dans le bon container
docker exec -it action_devcontainer-action-1 bash
cd /workspace && python -m rasa_sdk --actions src.actions
```

### Configuration rÃ©seau container-Ã -container
Assurez-vous que la variable `ACTION_ENDPOINT_URL` utilise le bon nom de service :
```bash
# âœ… Correct (nom du service Docker)
ACTION_ENDPOINT_URL=http://action:5055/webhook

# âŒ Incorrect (IP hardcodÃ©e du mauvais container)
ACTION_ENDPOINT_URL=http://172.18.0.6:5055/webhook
```

**ProblÃ¨me frÃ©quent** : Si votre container Rasa ne reÃ§oit pas les rÃ©ponses du serveur d'actions, vÃ©rifiez que `ACTION_ENDPOINT_URL` pointe vers le bon container :

```bash
# Diagnostic rapide
docker exec rasa_devcontainer-rasa-1 env | grep ACTION_ENDPOINT_URL
docker exec rasa_devcontainer-rasa-1 curl http://action:5055/health

# Si Ã§a ne fonctionne pas, corrigez la variable dans votre docker-compose.yml
# et redÃ©marrez le container Rasa
```

### Erreur "Ollama not accessible"
```bash
# VÃ©rifier Ollama
curl http://localhost:11434/api/tags

# Dans Docker, vÃ©rifier la connectivitÃ© rÃ©seau
curl http://ollama:11434/api/tags
```

### Timeout lors de la gÃ©nÃ©ration
- Utiliser un modÃ¨le plus petit (`llama3.2:1b` au lieu de `llama3.1:8b`)
- Augmenter le timeout dans les requÃªtes
- VÃ©rifier les ressources systÃ¨me (RAM/CPU)

### Erreurs de parsing JSON
Les erreurs de validation Pydantic sont normales avec Ollama - le systÃ¨me retourne automatiquement du JSON brut utilisable.

## ğŸ¤ Contribution

### Ajouter un nouveau type de mÃ©trique
1. Modifier `/src/shared/SSOT/MetricType.yml`
2. Ajouter des exemples dans `/src/langchain/planner_examples.py`
3. Tester avec les scripts de test

### AmÃ©liorer les prompts
Modifier les templates dans `/src/langchain/planner_chain.py` pour amÃ©liorer la qualitÃ© des rÃ©ponses.

## ğŸ“ Logs et Debugging

### Activer les logs dÃ©taillÃ©s
```bash
export PYTHONPATH=/workspace
python -m rasa_sdk --actions src.actions --debug
```

### Logs importants Ã  surveiller
- `Creating Ollama LLM` : Confirmation de l'utilisation d'Ollama
- `Parsed JSON successfully` : Parsing rÃ©ussi des rÃ©ponses LLM
- `Returning raw JSON` : Mode de compatibilitÃ© Ollama activÃ©

## ğŸ“„ Licence

[Ajouter la licence appropriÃ©e]

## ğŸ†˜ Support

Pour obtenir de l'aide :
1. VÃ©rifier les logs avec `--debug`
2. Tester la connectivitÃ© Ollama
3. Utiliser les scripts de test fournis
4. Consulter la section dÃ©pannage ci-dessus

---

**Note** : Ce projet est optimisÃ© pour Ollama et Ã©vite dÃ©libÃ©rÃ©ment les dÃ©pendances cloud comme OpenAI pour un contrÃ´le total et une confidentialitÃ© des donnÃ©es.
