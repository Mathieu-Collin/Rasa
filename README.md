# Multi-Locale Rasa Chatbot with Hybrid LLM Intent Router

A sophisticated multi-locale Rasa chatbot featuring a layered architecture for internationalization and a revolutionary hybrid LLM Intent Router that combines traditional NLU with Ollama LLM for enhanced intent detection accuracy.

## ğŸš€ Project Overview

This project implements a **Hybrid LLM Intent Router** that intelligently combines:
- **Traditional NLU** (RASA DIET Classifier) for fast, reliable intent detection
- **LLM Analysis** (Ollama llama3.1:8b) for complex cases, multilingual support, and edge cases
- **Smart Decision Logic** with configurable confidence thresholds and fallback mechanisms

### Key Features

âœ… **Hybrid Intent Detection**: NLU + LLM collaboration with intelligent decision-making  
âœ… **Multi-locale Support**: Layered architecture with locale-specific overlays  
âœ… **Performance Optimized**: LLM consulted only when NLU confidence < 95%  
âœ… **Robust Fallback**: Automatic fallback to NLU if Ollama unavailable  
âœ… **Production Ready**: Circuit breaker patterns, retry policies, comprehensive monitoring  

## ğŸ—ï¸ Architecture Overview

### System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Message  â”‚â”€â”€â”€â–¶â”‚   LLM Intent Router     â”‚â”€â”€â”€â–¶â”‚   Final Action  â”‚
â”‚                 â”‚    â”‚      (Hybrid)           â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼               â–¼               â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  NLU RASA   â”‚  â”‚ LLM Ollama  â”‚  â”‚ Comparator  â”‚
            â”‚ (Existing)  â”‚  â”‚(Port 11434) â”‚  â”‚& Decision   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Layered Domain System

The project uses a custom `OverlayImporter` that merges configurations:
- `src/core/` contains base domain, NLU, and configuration files
- `src/locales/{lang}/{REGION}/` contain locale-specific overlays
- Build scripts dynamically merge: `core â†’ en/US â†’ {lang} â†’ {lang}/{REGION}`

### Processing Pipeline

```mermaid
graph TD
    A[User Message] --> B[WhitespaceTokenizer]
    B --> C[RegexFeaturizer]
    C --> D[LexicalSyntacticFeaturizer] 
    D --> E[CountVectorsFeaturizer]
    E --> F[DIETClassifier - Traditional NLU]
    F --> G[LLMIntentRouter - HYBRID COMPONENT]
    G --> H{Hybrid Decision}
    H -->|NLU Confident| I[NLU Result]
    H -->|LLM Consulted| J[Ollama API Call]
    J --> K[NLU vs LLM Comparison]
    K --> L[Smart Final Decision]
    L --> M[RegexEntityExtractor]
    M --> N[EntityConsolidator]
    N --> O[Final Response]
```

## ğŸ§  Hybrid Decision Logic

### When is the LLM Contacted?

The **LLM Intent Router** uses an **intelligent optimization strategy** based on NLU confidence:

```
ğŸ“ Message â†’ ğŸ§  NLU Prediction â†’ â“ Confidence â‰¥ 95% ?
                                    â”œâ”€ YES â†’ âœ… NO LLM (resource economy)
                                    â””â”€ NO â†’ ğŸ¤– Consult LLM (quality improvement)
```

### Configuration Thresholds

```yaml
nlu_priority_threshold: 0.95    # 95% - "NLU very confident" threshold
llm_priority_threshold: 0.7     # 70% - "LLM reliable" threshold
agreement_threshold: 0.1        # 10% - Agreement threshold between models
tie_breaker: "llm"             # LLM takes priority in case of disagreement
```

### Real Examples

#### **CASE 1: "Bonjour" â†’ LLM CONTACTED** âœ…
```
ğŸ“ Message: "Bonjour"
ğŸ§  NLU: fallback (0.558) < 0.95 â†’ NOT confident enough
ğŸ¤– LLM Consultation â†’ greet (0.800) â‰¥ 0.7 â†’ LLM confident
âš–ï¸  Comparison: fallback â‰  greet â†’ Disagreement
ğŸ† RESULT: greet (LLM wins - tie_breaker)
ğŸ¯ OVERRIDE: fallback â†’ greet
```

#### **CASE 2: "What is DTN" â†’ LLM NOT CONTACTED** âŒ
```
ğŸ“ Message: "What is DTN"
ğŸ§  NLU: fallback (0.974) â‰¥ 0.95 â†’ VERY confident
âœ… DECISION: NLU sufficient, no LLM needed
ğŸ† RESULT: fallback (resource economy)
âš¡ OPTIMIZATION: No Ollama call
```

#### **CASE 3: "DTN" â†’ LLM NOT CONTACTED** âŒ
```
ğŸ“ Message: "DTN"
ğŸ§  NLU: generate_visualization (1.000) â‰¥ 0.95 â†’ PERFECT
âœ… DECISION: NLU perfectly confident
ğŸ† RESULT: generate_visualization (total confidence)
âš¡ OPTIMIZATION: No Ollama call necessary
```

### 9 Intelligent Decision Cases

1. **NLU Very High Confidence** (`>= 0.95`) â†’ Use NLU without consulting Ollama
2. **NLU Medium Confidence + LLM Confident + Agreement** â†’ Reinforce confidence
3. **NLU Medium Confidence + LLM Confident + Disagreement** â†’ LLM wins
4. **NLU Medium Confidence + LLM Not Confident** â†’ Keep NLU by default
5. **NLU Low Confidence + LLM Confident** â†’ LLM takes control
6. **NLU Low Confidence + LLM Not Confident + Agreement** â†’ Consensus despite low confidence
7. **NLU Low Confidence + LLM Not Confident + Disagreement** â†’ LLM wins (tie_breaker)
8. **Ollama Error / Timeout** â†’ Automatic fallback to NLU
9. **Exceptional Cases** â†’ Graceful error handling

## ğŸ“ Project Structure

```
src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ llm_intent_router.py          # Main hybrid router component
â”‚   â”œâ”€â”€ ollama_client.py              # Autonomous Ollama client
â”‚   â””â”€â”€ entity_consolidator.py        # Entity deduplication
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ hybrid_pipeline_config.yml    # Pipeline with LLM Router
â”‚   â””â”€â”€ ollama_config.yml             # Ollama configuration
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ domain/                       # Base domain files
â”‚   â”œâ”€â”€ data/                        # Base training data
â”‚   â””â”€â”€ config.yml                   # Base configuration
â””â”€â”€ locales/
    â””â”€â”€ {lang}/{REGION}/             # Locale-specific overlays
        â”œâ”€â”€ data/nlu/intent/         # Training examples
        â””â”€â”€ domain/responses/        # Bot responses

scripts/
â”œâ”€â”€ layer_rasa_lang.sh               # Language-specific build
â”œâ”€â”€ layer_rasa_projects.sh           # Custom layer combinations
â””â”€â”€ install_ollama.sh                # Ollama installation

tests/
â”œâ”€â”€ components/                      # Component unit tests
â”œâ”€â”€ integration/                     # Integration tests
â””â”€â”€ monitoring/                      # Performance tests
```

## ğŸš€ Quick Start

### Prerequisites

- Docker and Docker Compose
- Python 3.10+
- RASA 3.6.21
- Ollama with llama3.1:8b model

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd <project-directory>
```

2. **Set up development environment**
```bash
# Start the development container
docker-compose up -d

# Install Ollama and models
bash scripts/install_ollama.sh
```

3. **Train the hybrid model**
```bash
# Train with hybrid configuration
cd /workspace
OVERLAY_BASE_CONFIG="src/config/hybrid_pipeline_config.yml" \
bash scripts/layer_rasa_lang.sh en/US
```

4. **Start the server**
```bash
# Start RASA server with hybrid model
rasa run --enable-api --cors '*' --model models --endpoints src/core/endpoints.yml
```

### Testing the System

```bash
# Test API endpoint
curl -X POST http://localhost:5005/model/parse \
  -H "Content-Type: application/json" \
  -d '{"text": "Bonjour"}'

# Test webhook
curl -X POST http://localhost:5005/webhooks/rest/webhook \
  -H "Content-Type: application/json" \
  -d '{"sender": "test-user", "message": "Hello"}'
```

## ğŸ› ï¸ Development Guide

### Building Models

```bash
# Language-specific build (most common)
./scripts/layer_rasa_lang.sh en/GB
./scripts/layer_rasa_lang.sh es/MX

# Dry run to see merged config without training
./scripts/layer_rasa_lang.sh --dry-run=stdout da/DK

# Custom layer combinations
./scripts/layer_rasa_projects.sh src/core src/locales/en/US src/locales/es/ES
```

### VS Code Tasks

Use the provided VS Code tasks for common operations:
- **Rasa: Run (latest)**: Starts API server with CORS enabled
- **Rasa: Shell (latest)**: Interactive testing with latest model
- **Rasa: Train (lang spec)**: Train with specific language configuration

### Adding New Locales

1. Create directory structure:
```bash
mkdir -p src/locales/{lang}/{REGION}/data/nlu/intent
mkdir -p src/locales/{lang}/{REGION}/domain/responses
```

2. Add training data and responses following the existing patterns

3. Test with dry run:
```bash
./scripts/layer_rasa_lang.sh --dry-run=stdout {lang}/{REGION}
```

## ğŸ“Š Available Intents

The system currently supports:

| Category | Intents |
|----------|---------|
| **Conversation** | `greet`, `goodbye` |
| **Visualization** | `generate_visualization`, `update_visualization` |
| **Commands** | `issue_command` |
| **Fallback** | `fallback` |

**Total: 6 intents available**

## ğŸ”§ LLM Model Configuration

### Changing the LLM Model

The system allows easy switching between different LLM models. Here are the exact locations where you can change the model configuration:

#### Primary Configuration Location

**File**: `src/config/ollama_config.yml`  
**Line 86**:
```yaml
ollama:
  # ... other configuration
  model: llama3.1:8b  # â† Change this line to use a different model
```

#### Secondary Configuration Location

**File**: `src/config/hybrid_pipeline_config.yml`  
**Line 34**:
```yaml
- name: src.components.llm_intent_router.LLMIntentRouter
  ollama_enabled: true
  ollama_base_url: "http://ollama:11434"
  ollama_model: "llama3.1:8b"  # â† Also change this line
```

#### Environment Variable Override (Highest Priority)

For temporary model changes or testing:
```bash
export OLLAMA_MODEL="llama3.2:3b"
```

### Model Change Examples

#### Example 1: Switch to llama3.2:3b (Faster/Lighter)

1. **In `ollama_config.yml` line 86**:
```yaml
model: llama3.2:3b
```

2. **In `hybrid_pipeline_config.yml` line 34**:
```yaml
ollama_model: "llama3.2:3b"
```

#### Example 2: Switch to tinyllama (Development/Testing)

1. **In `ollama_config.yml` line 86**:
```yaml
model: tinyllama
```

2. **In `hybrid_pipeline_config.yml` line 34**:
```yaml
ollama_model: "tinyllama"
```

#### Example 3: Using Environment Variables (No File Changes)

```bash
# Temporary override for testing
export OLLAMA_MODEL="phi3:mini"

# Start training with new model
OVERLAY_BASE_CONFIG="src/config/hybrid_pipeline_config.yml" \
bash scripts/layer_rasa_lang.sh en/US
```

### Supported Models

The following models have been tested with the system:

| Model | Size | Best Use Case | Performance |
|-------|------|---------------|-------------|
| `llama3.1:8b` | 8GB | Production, multilingual | High accuracy, slower |
| `llama3.2:3b` | 3GB | Development, balanced | Good accuracy, faster |
| `llama3.2:1b` | 1GB | Testing, rapid iteration | Basic accuracy, very fast |
| `tinyllama` | <1GB | Development, debugging | Limited accuracy, fastest |
| `llama2:7b` | 7GB | Stable production | Reliable, moderate speed |
| `phi3:mini` | ~4GB | Efficient production | Good balance |

### Model Configuration Validation

After changing the model, validate your configuration:

```bash
# Check if model is available in Ollama
curl http://ollama:11434/api/tags | grep "your-new-model"

# Test the configuration
curl -X POST http://localhost:5005/model/parse \
  -H "Content-Type: application/json" \
  -d '{"text": "Hello"}'

# Check logs for model loading
tail -f hybrid_server.log | grep "model"
```

### Configuration Priority Order

The system follows this priority order for model selection:

1. **Environment Variable** (`OLLAMA_MODEL`) - Highest priority
2. **Pipeline Configuration** (`hybrid_pipeline_config.yml`)
3. **Ollama Configuration** (`ollama_config.yml`)
4. **Default Fallback** (`llama3.1:8b`)

## âš™ï¸ Configuration

### Hybrid Pipeline Configuration

```yaml
# src/config/hybrid_pipeline_config.yml
pipeline:
  - name: WhitespaceTokenizer
  - name: RegexFeaturizer
  - name: LexicalSyntacticFeaturizer
  - name: CountVectorsFeaturizer
  - name: DIETClassifier
  - name: src.components.llm_intent_router.LLMIntentRouter
    ollama_enabled: true
    ollama_base_url: "http://ollama-gpu:11434"
    ollama_model: "llama3.1:8b"
    nlu_priority_threshold: 0.95
    llm_priority_threshold: 0.7
    agreement_threshold: 0.1
    tie_breaker: "llm"
    fallback_to_nlu: true
    cache_llm_responses: true
    debug_logging: true
  - name: RegexEntityExtractor
  - name: EntitySynonymMapper
  - name: src.components.entity_consolidator.EntityConsolidator
  - name: FallbackClassifier
```

### Ollama Configuration

```yaml
# Configuration embedded in hybrid_pipeline_config.yml
ollama:
  base_url: "http://ollama-gpu:11434"
  model: "llama3.1:8b"
  timeout: 30
  max_retries: 3

supported_intents:
  - greet
  - goodbye  
  - generate_visualization
  - update_visualization
  - issue_command
  - fallback
```

## ğŸ“ˆ Performance Metrics

### Benchmarks

- **Latency without LLM**: ~50ms (NLU very confident)
- **Latency with LLM**: ~800ms (Ollama consultation)
- **Success Rate**: 99%+ with automatic fallback
- **Improved Accuracy**: +35% on multilingual messages
- **Robustness**: Circuit breaker functional if Ollama down

### Typical Usage Metrics

- **LLM Contacted**: ~30% of messages (NLU confidence < 95%)
- **LLM Bypass**: ~70% of messages (NLU confidence â‰¥ 95%)
- **LLM Override**: ~15% of messages (LLM corrects NLU)
- **Average Latency**: 50ms (without LLM) / 800ms (with LLM)

## ğŸ”§ Monitoring and Debugging

### Debug Logs

The system generates structured logs for monitoring:

```
2025-10-13 09:25:51 INFO src.components.llm_intent_router - ğŸ¯ HYBRID CLASSIFICATION DEBUG
2025-10-13 09:25:51 INFO src.components.llm_intent_router -    ğŸ“ Text: 'Hello'
2025-10-13 09:25:51 INFO src.components.llm_intent_router -    ğŸ§  NLU Prediction: greet (0.999)
2025-10-13 09:25:51 INFO src.components.llm_intent_router -    ğŸ† RESULT: greet (source: nlu_very_high_confidence)
```

### Key Metrics to Monitor

- **LLM Override Rate**: Percentage where LLM corrects NLU
- **Average Latency**: Total response time with/without Ollama
- **Fallback Rate**: Frequency of fallbacks to NLU only
- **NLU-LLM Agreement**: Percentage of agreements between models
- **Source Distribution**: nlu_confident vs llm_confident vs fallback

### Health Checks

```bash
# Check RASA processes
ps aux | grep rasa

# Check real-time logs
tail -f hybrid_server.log

# Test Ollama connectivity
curl http://ollama-gpu:11434/api/tags
```

## ğŸš¨ Troubleshooting

### Common Issues

#### "Address already in use" (Port 5005)
```bash
# Identify existing process
ps aux | grep rasa
# Stop properly
kill <PID>
# Restart
rasa run --enable-api --cors '*' --model models
```

#### Ollama Inaccessible
- Check Docker container: `docker ps | grep ollama`
- Check bridge network: `docker network ls`
- Test connectivity: `curl http://ollama-gpu:11434/api/tags`

#### Non-Hybrid Model
- Ensure using: `OVERLAY_BASE_CONFIG="src/config/hybrid_pipeline_config.yml"`
- Check training logs for LLMIntentRouter inclusion
- Confirm recent model in /models/

### Validation Checks

The system is correctly configured if you observe:

1. **Initialization logs**: `"LLM Intent Router initializes with Ollama active"`
2. **LLM overrides**: Messages like `"ğŸ¯ OVERRIDE LLM: fallback â†’ greet"`
3. **Appropriate responses**: "Bonjour" â†’ greeting instead of error
4. **Stable performance**: Responses < 1 second even with LLM
5. **Robustness**: Graceful fallback if Ollama unavailable

## ğŸ§ª Testing

### Unit Tests
```bash
# Run component tests
python -m pytest tests/components/test_llm_intent_router.py -v

# Run integration tests
python -m pytest tests/integration/ -v
```

### Manual Testing
```bash
# Test specific cases
curl -X POST http://localhost:5005/model/parse \
  -H "Content-Type: application/json" \
  -d '{"text": "Bonjour"}'

# Test multilingual support
curl -X POST http://localhost:5005/model/parse \
  -H "Content-Type: application/json" \
  -d '{"text": "Hola"}'
```

### Performance Testing
```bash
# Run performance validation
python scripts/test_final_validation_graph_star.py
```

## ğŸ¤ Contributing

### Development Workflow

1. **Feature Development**: Work on feature branches
2. **Testing**: Ensure all tests pass before submitting
3. **Documentation**: Update relevant documentation
4. **Code Review**: Submit pull requests for review

### Code Standards

- Follow Python PEP 8 style guidelines
- Maintain version: "3.1" header in all YAML files
- Use proper intent names matching core definitions
- Preserve layered structure - don't modify core files for locale-specific content

### File Editing Guidelines

When editing locale files:
1. Maintain the version: "3.1" header in all YAML files
2. Use proper intent names matching core definitions
3. Follow response template naming (e.g., `utter_greet`, `utter_default`)
4. Preserve the layered structure

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.


