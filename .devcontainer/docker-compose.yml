services:
  rasa:
    build:
      context: ..
      dockerfile: .devcontainer/Dockerfile
    volumes:
      - ..:/workspace
    depends_on:
      - duckling
      - redis
      - ollama
    ports:
      - "6005:6005"
    env_file:
      - .env
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - rasa-network
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434

  redis:
    image: redis:alpine
    command: ["redis-server", "--save", "20", "1", "--loglevel", "warning"]
    networks:
      - rasa-network

  duckling:
    image: rasa/duckling:latest
    networks:
      - rasa-network

  ollama:
    image: ollama/ollama:latest
    container_name: Ollama_devcontainer
    ports:
      - "11434:11434"
    volumes:
      # Use COMPOSE_PROJECT_DIR which is set by docker compose to the
      # directory containing the compose file. This avoids needing an
      # external PROJ_ROOT environment variable.
      - ./ollama_save:/root/.ollama
    networks:
      - rasa-network
    restart: unless-stopped
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
      - OLLAMA_GPU_LAYERS=999
      - OLLAMA_FLASH_ATTENTION=1
      - OLLAMA_HOST=0.0.0.0:11434
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

networks:
  rasa-network:
    name: devcontainer_rasa-network
    driver: bridge
## Persist Ollama data in a bind-mounted folder at the project root.
## The folder will be ${COMPOSE_PROJECT_DIR}/ollama_save when using docker compose.
## If you prefer a Docker named volume, replace the bind-mount above with a
## named volume and declare it here (without external: true) so Docker will
## create it automatically.
