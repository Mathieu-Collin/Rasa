#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de test pour valider le systÃ¨me de debug logging du LLM Intent Router
Teste diffÃ©rents scÃ©narios et affiche les logs    try:
        router = LLMIntentRouter.create(
            config=config['llm_intent_router'],
            model_storage=None,
            resource=None,
            execution_context=None
        )lÃ©s pour chaque dÃ©cision
"""

import logging
import os
import sys
import tempfile
from pathlib import Path

import yaml

# Ajouter le workspace au PYTHONPATH
workspace_path = Path(__file__).parent.parent
sys.path.insert(0, str(workspace_path))

from src.components.llm_intent_router import LLMIntentRouter


def setup_debug_logging():
    """Configure le logging pour afficher les messages de debug"""
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[logging.StreamHandler(sys.stdout)],
    )


def create_test_config():
    """CrÃ©e une configuration de test avec debug activÃ©"""
    config = {
        "llm_intent_router": {
            "debug_logging": True,  # DEBUG ACTIVÃ‰
            "ollama_enabled": True,
            "ollama_base_url": "http://172.22.0.2:11434",  # IP du bridge rÃ©seau
            "ollama_model": "llama3.2:1b",
            "ollama_config_path": "src/config/ollama_config.yml",
            "nlu_priority_threshold": 0.8,
            "llm_priority_threshold": 0.9,
            "agreement_threshold": 0.1,
            "fallback_to_nlu": True,
            "cache_enabled": True,
            "cache_ttl": 300,
            "timeout": 30,
            "max_retries": 3,
        }
    }

    # CrÃ©er un fichier de config temporaire
    with tempfile.NamedTemporaryFile(mode="w", suffix=".yml", delete=False) as f:
        yaml.dump(config, f, default_flow_style=False)
        return f.name


def create_mock_message(text: str, nlu_intent: str, nlu_confidence: float):
    """CrÃ©e un message de test avec intention NLU simulÃ©e"""

    class MockMessage:
        def __init__(self, text: str, intent: str, confidence: float):
            self._data = {
                "text": text,
                "intent": {"name": intent, "confidence": confidence},
            }

        def get(self, key: str, default=None):
            return self._data.get(key, default)

        def set(self, key: str, value):
            self._data[key] = value

    return MockMessage(text, nlu_intent, nlu_confidence)


def test_debug_scenarios():
    """Teste diffÃ©rents scÃ©narios avec debug logging activÃ©"""

    print("ðŸš€ DÃ‰MARRAGE DES TESTS DEBUG LOGGING")
    print("=" * 80)

    setup_debug_logging()

    # CrÃ©er la config de test
    config_file = create_test_config()

    try:
        # Initialiser le routeur avec debug activÃ©
        config_data = yaml.safe_load(open(config_file))["llm_intent_router"]
        router = LLMIntentRouter.create(
            config=config_data,  # Passer directement la config du component
            model_storage=None,
            resource=None,
            execution_context=None,
        )

        print(f"âœ… Routeur initialisÃ© avec debug_logging = {router._debug_logging}")
        print("\n")

        # ScÃ©nario 1: NLU trÃ¨s confiant (>= 0.8)
        print("ðŸ§ª SCÃ‰NARIO 1: NLU HAUTE CONFIANCE")
        print("-" * 50)
        message1 = create_mock_message("Bonjour", "greet", 0.95)
        router.process([message1])
        print("\n")

        # ScÃ©nario 2: NLU moins confiant (< 0.8)
        print("ðŸ§ª SCÃ‰NARIO 2: NLU CONFIANCE MOYENNE")
        print("-" * 50)
        message2 = create_mock_message(
            "Je veux faire un graphique", "visualization", 0.65
        )
        router.process([message2])
        print("\n")

        # ScÃ©nario 3: Message ambigu
        print("ðŸ§ª SCÃ‰NARIO 3: MESSAGE AMBIGU")
        print("-" * 50)
        message3 = create_mock_message("Aide moi", "help", 0.50)
        router.process([message3])
        print("\n")

        # ScÃ©nario 4: Commande plus complexe
        print("ðŸ§ª SCÃ‰NARIO 4: COMMANDE COMPLEXE")
        print("-" * 50)
        message4 = create_mock_message(
            "Peux-tu crÃ©er un graphique en barres des ventes par rÃ©gion",
            "visualization",
            0.70,
        )
        router.process([message4])
        print("\n")

        # Afficher les statistiques finales
        print("ðŸ“Š STATISTIQUES FINALES")
        print("-" * 50)
        stats = router.get_stats()
        for key, value in stats.items():
            print(f"   {key}: {value}")

        print("\nâœ… TESTS DEBUG TERMINÃ‰S AVEC SUCCÃˆS")

    except Exception as e:
        print(f"âŒ ERREUR DURANT LES TESTS: {e}")
        import traceback

        traceback.print_exc()

    finally:
        # Nettoyer le fichier temporaire
        try:
            os.unlink(config_file)
        except Exception:
            pass


def test_ollama_unavailable():
    """Teste le comportement quand Ollama n'est pas disponible"""
    print("\nðŸ§ª TEST SUPPLÃ‰MENTAIRE: OLLAMA INDISPONIBLE")
    print("-" * 50)

    # CrÃ©er config avec Ollama dÃ©sactivÃ©
    config = {
        "llm_intent_router": {
            "debug_logging": True,
            "ollama_enabled": False,  # OLLAMA DÃ‰SACTIVÃ‰ pour ce test
            "ollama_base_url": "http://172.22.0.2:11434",  # IP du bridge rÃ©seau
            "nlu_priority_threshold": 0.8,
            "fallback_to_nlu": True,
        }
    }

    with tempfile.NamedTemporaryFile(mode="w", suffix=".yml", delete=False) as f:
        yaml.dump(config, f, default_flow_style=False)
        config_file = f.name

    try:
        router = LLMIntentRouter.create(
            config={"llm_intent_router": config["llm_intent_router"]},
            model_storage=None,
            resource=None,
            execution_context=None,
        )

        message = create_mock_message("Test sans Ollama", "test", 0.60)
        router.process([message])

    finally:
        os.unlink(config_file)


if __name__ == "__main__":
    test_debug_scenarios()
    test_ollama_unavailable()
