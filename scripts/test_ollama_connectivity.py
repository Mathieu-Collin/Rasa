#!/usr/bin/env python3
"""
Script de test pour trouver l'URL cor    # URLs √† tester dans l'ordre de pr√©f√©rence
    urls_to_test = [
        "http://172.22.0.2:11434",     # IP du bridge r√©seau (PRIORIT√â)
        "http://localhost:11434",
        "http://127.0.0.1:11434",
        "http://host.docker.internal:11434",
        "http://192.168.65.254:11434",  # IP de host.docker.internal
        "http://172.17.0.1:11434",     # Gateway Docker par d√©faut
        "http://172.20.0.1:11434",     # Gateway trouv√©e plus t√¥t
        "http://10.0.2.2:11434",       # Gateway VirtualBox/VMware
    ] Ollama depuis le devcontainer
Teste plusieurs configurations possibles
"""

import sys
from pathlib import Path

import requests

# Ajouter le workspace au PYTHONPATH
workspace_path = Path(__file__).parent.parent
sys.path.insert(0, str(workspace_path))


def test_ollama_url(url: str, timeout: int = 10) -> bool:
    """Teste une URL Ollama"""
    try:
        print(f"üß™ Test: {url}")
        response = requests.get(f"{url}/api/version", timeout=timeout)
        if response.status_code == 200:
            version_data = response.json()
            print(f"   ‚úÖ SUCC√àS! Version: {version_data.get('version', 'unknown')}")
            return True
        else:
            print(f"   ‚ùå Code erreur: {response.status_code}")
            return False
    except requests.exceptions.ConnectTimeout:
        print(f"   ‚è±Ô∏è  Timeout de connexion ({timeout}s)")
        return False
    except requests.exceptions.ConnectionError as e:
        print(f"   üîå Erreur connexion: {str(e)[:100]}...")
        return False
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")
        return False


def test_ollama_tags(url: str, timeout: int = 10) -> bool:
    """Teste l'endpoint /api/tags pour voir les mod√®les disponibles"""
    try:
        print(f"üìã Test mod√®les: {url}/api/tags")
        response = requests.get(f"{url}/api/tags", timeout=timeout)
        if response.status_code == 200:
            tags_data = response.json()
            models = [
                model.get("name", "unknown") for model in tags_data.get("models", [])
            ]
            print(f"   ‚úÖ Mod√®les disponibles: {models}")
            return True
        else:
            print(f"   ‚ùå Code erreur: {response.status_code}")
            return False
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")
        return False


def main():
    """Teste diff√©rentes URLs pour Ollama"""
    print("üîç RECHERCHE DE LA CONFIGURATION OLLAMA CORRECTE")
    print("=" * 60)

    # URLs √† tester dans l'ordre de pr√©f√©rence
    urls_to_test = [
        "http://localhost:11434",
        "http://127.0.0.1:11434",
        "http://host.docker.internal:11434",
        "http://192.168.65.254:11434",  # IP de host.docker.internal
        "http://172.17.0.1:11434",  # Gateway Docker par d√©faut
        "http://172.20.0.1:11434",  # Gateway trouv√©e plus t√¥t
        "http://10.0.2.2:11434",  # Gateway VirtualBox/VMware
    ]

    working_urls = []

    for url in urls_to_test:
        print()
        if test_ollama_url(url, timeout=5):
            working_urls.append(url)
            # Tester aussi les mod√®les disponibles
            test_ollama_tags(url, timeout=10)

    print("\n" + "=" * 60)
    if working_urls:
        print("üéâ URLs FONCTIONNELLES TROUV√âES:")
        for url in working_urls:
            print(f"   ‚úÖ {url}")

        print("\nüìù RECOMMANDATION:")
        print(f"   Utilisez: {working_urls[0]}")

        # Mise √† jour automatique de la configuration
        try:
            print("\nüîß MISE √Ä JOUR DE LA CONFIGURATION...")
            update_config_file(working_urls[0])
            print("   ‚úÖ Configuration mise √† jour dans src/config/ollama_config.yml")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Impossible de mettre √† jour automatiquement: {e}")
    else:
        print("‚ùå AUCUNE URL FONCTIONNELLE TROUV√âE")
        print("\nüí° SOLUTIONS POSSIBLES:")
        print("   1. V√©rifiez que le conteneur Ollama est d√©marr√©")
        print("   2. V√©rifiez les ports expos√©s du conteneur")
        print("   3. Essayez de red√©marrer le conteneur Ollama")
        print("   4. V√©rifiez la configuration r√©seau Docker")


def update_config_file(working_url: str):
    """Met √† jour le fichier de configuration avec l'URL qui fonctionne"""
    import yaml

    config_path = Path(__file__).parent.parent / "src/config/ollama_config.yml"

    if config_path.exists():
        with open(config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)

        config["ollama"]["base_url"] = working_url

        with open(config_path, "w", encoding="utf-8") as f:
            yaml.dump(config, f, default_flow_style=False, allow_unicode=True)


if __name__ == "__main__":
    main()
